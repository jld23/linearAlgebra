{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook I'll illistrate how text analyics can be done using Python and public models\n",
    "\n",
    "**There are a set of exercises to do by hand, followed by code. We love computers because they add faster than we do and never get tiered, but it is essential that you do the hand exercies to fully understand what the computer is doing.**\n",
    "\n",
    "The main work behind this is to take text information and convert it to numerical values so that a computer can perform tasks similar to humans.\n",
    "These tasks include:\n",
    "\n",
    "*  Finding similar words\n",
    "*  Clustering documents\n",
    "*  Natural Language Processing (NLP) \n",
    "    *  Sentiment analysis\n",
    "    *  Language translation\n",
    "    *  Photo captioning\n",
    "\n",
    "NLP is a very broad topic and this is just an introduction for more information start [here](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "Many text models are based on [GloVe](https://nlp.stanford.edu/projects/glove/) and [word2vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "\n",
    "## Word2vec\n",
    "\n",
    "Word2vec was orgionally published in 2013 by Tomas Mikolov and patented while he was working at Google. You can build your own `word2vec` model on any corpus of text but my recommendation is to use a pretrained model. These are usually based on very large collection of text like Newsgroups, quora, or wikipedia.\n",
    "`gensim` is a very popular Python package for using prebuilt language models \n",
    "\n",
    "\n",
    "\n",
    "## GloVe\n",
    "\n",
    "GloVe is a collection of models that were trained on different corpus. The most common was trained on Wikipedia and includes 6 billion tokens and 400k words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram from [Adam Geitgey](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e) that illistrates the steps of NLP. Many people break the steps down in slighly different ways.\n",
    "\n",
    "\n",
    "![NLP Pipeline](./NLP_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "This step takes a document (sentence in our case) and converts it to a numerical form as well as counts the word frequency.\n",
    "\n",
    "Here is an a paragraph about [Geoffrey Hinton's education](https://en.wikipedia.org/wiki/Geoffrey_Hinton) from Wikipedia:\n",
    "\n",
    "\"Hinton was educated at King's College, Cambridge, graduating in 1970 with a Bachelor of Arts in experimental psychology. He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.\"\n",
    "\n",
    "\n",
    "1. Break the paragraph into sentences.\n",
    "    *  Hinton was educated at King's College, Cambridge, graduating in 1970 with a Bachelor of Arts in experimental psychology.\n",
    "    *  He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.\n",
    "\n",
    "**I'll show you with the first sentence and you need do the second sentence.**\n",
    "\n",
    "1. Take the sentence and break it into word tokens:\n",
    "    \"Hinton\", \"was\", \"educated\", \"at\", \"King's\", \"College\", \"Cambridge\", \"graduating\", \"in\", \"1970\", \"with\", \"a\", \"Bachelor\", \"of\", \"Arts\", \"in\", \"experimental\", \"psychology\", \".\"\n",
    "\n",
    "1. Count the frequency of each word (token)\n",
    "    bag_of_words1 = {'Mason':1, 'likes':1, 'to':1, 'learn':1, 'about':1, 'computers':1}\n",
    "\n",
    "    bag_of_words2 = {}\n",
    "\n",
    "1. Now combine the two sentences **hint:** 'Mason':1 & 'likes':2\n",
    "\n",
    "    `bag_of_words1 + bag_of_words2 = bag_of_words3`\n",
    "\n",
    "    bag_of_words3 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the value (answer() of `bag_of_words3`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /Users/jadean/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/jadean/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the text into sentences and then tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dictionary(37 unique tokens: ['1970', '1978', 'Arts', 'Bachelor', 'Cambridge,']...)\n"
    }
   ],
   "source": [
    "paragraph = [\"Hinton was educated at King's College, Cambridge, graduating in 1970 with a Bachelor of Arts in experimental psychology. He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.\"]\n",
    "\n",
    "texts = [[text for text in doc.split()] for doc in paragraph]\n",
    "word_dict = corpora.Dictionary(texts)\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the unique id's for each token (word) in the text.\n",
    "\n",
    "You can see that is sorted alphabetically. Every unique word is in here -- including 'He' and 'he'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'1970': 0, '1978': 1, 'Arts': 2, 'Bachelor': 3, 'Cambridge,': 4, 'Christopher': 5, 'College,': 6, 'Edinburgh': 7, 'He': 8, 'Hinton': 9, \"King's\": 10, 'Longuet-Higgins.': 11, 'PhD': 12, 'University': 13, 'a': 14, 'artificial': 15, 'at': 16, 'awarded': 17, 'by': 18, 'continued': 19, 'educated': 20, 'experimental': 21, 'for': 22, 'graduating': 23, 'he': 24, 'his': 25, 'in': 26, 'intelligence': 27, 'of': 28, 'psychology.': 29, 'research': 30, 'study': 31, 'supervised': 32, 'the': 33, 'was': 34, 'where': 35, 'with': 36}\n"
    }
   ],
   "source": [
    "print(word_dict.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a pretrained word2vec model on the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_address = \"\"\"Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n",
    "\n",
    "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are met on a great battle field of that war. We have come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live. This we may, in all propriety do. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow, this ground-- The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract. The world will little note, nor long remember what we say here; while it can never forget what they did here.\n",
    "\n",
    "It is rather for us, the living, to stand here, we here be dedicated to the great task remaining before us -- that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion -- that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people by the people for the people, shall not perish from the earth.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the speech into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_sentences = nltk.sent_tokenize(gettysburg_address)\n",
    "gettysburg_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the speech into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Four',\n 'score',\n 'and',\n 'seven',\n 'years',\n 'ago',\n 'our',\n 'fathers',\n 'brought',\n 'forth']"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "gettysburg_word_tokens = nltk.tokenize.word_tokenize(gettysburg_address)\n",
    "\n",
    "# Show the first 10 words\n",
    "gettysburg_word_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech Tagging\n",
    "\n",
    "Here is a list of the codes:\n",
    "```\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‘big’\n",
    "JJR adjective, comparative ‘bigger’\n",
    "JJS adjective, superlative ‘biggest’\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‘desk’\n",
    "NNS noun plural ‘desks’\n",
    "NNP proper noun, singular ‘Harrison’\n",
    "NNPS proper noun, plural ‘Americans’\n",
    "PDT predeterminer ‘all the kids’\n",
    "POS possessive ending parent’s\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO, to go ‘to’ the store.\n",
    "UH interjection, errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense, took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle is taken\n",
    "VBP verb, sing. present, known-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-adverb where, when\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Four', 'CD'),\n ('score', 'NN'),\n ('and', 'CC'),\n ('seven', 'CD'),\n ('years', 'NNS'),\n ('ago', 'RB'),\n ('our', 'PRP$'),\n ('fathers', 'NNS'),\n ('brought', 'VBD'),\n ('forth', 'NN'),\n (',', ','),\n ('upon', 'IN'),\n ('this', 'DT'),\n ('continent', 'NN'),\n (',', ','),\n ('a', 'DT'),\n ('new', 'JJ'),\n ('nation', 'NN'),\n (',', ','),\n ('conceived', 'VBN'),\n ('in', 'IN'),\n ('liberty', 'NN'),\n (',', ','),\n ('and', 'CC'),\n ('dedicated', 'VBD'),\n ('to', 'TO'),\n ('the', 'DT'),\n ('proposition', 'NN'),\n ('that', 'IN'),\n ('``', '``'),\n ('all', 'DT'),\n ('men', 'NNS'),\n ('are', 'VBP'),\n ('created', 'VBN'),\n ('equal', 'JJ'),\n (\"''\", \"''\"),\n ('.', '.'),\n ('Now', 'RB'),\n ('we', 'PRP'),\n ('are', 'VBP'),\n ('engaged', 'VBN'),\n ('in', 'IN'),\n ('a', 'DT'),\n ('great', 'JJ'),\n ('civil', 'JJ'),\n ('war', 'NN'),\n (',', ','),\n ('testing', 'VBG'),\n ('whether', 'IN'),\n ('that', 'DT'),\n ('nation', 'NN'),\n (',', ','),\n ('or', 'CC'),\n ('any', 'DT'),\n ('nation', 'NN'),\n ('so', 'RB'),\n ('conceived', 'JJ'),\n (',', ','),\n ('and', 'CC'),\n ('so', 'RB'),\n ('dedicated', 'JJ'),\n (',', ','),\n ('can', 'MD'),\n ('long', 'VB'),\n ('endure', 'NN'),\n ('.', '.'),\n ('We', 'PRP'),\n ('are', 'VBP'),\n ('met', 'VBN'),\n ('on', 'IN'),\n ('a', 'DT'),\n ('great', 'JJ'),\n ('battle', 'NN'),\n ('field', 'NN'),\n ('of', 'IN'),\n ('that', 'DT'),\n ('war', 'NN'),\n ('.', '.'),\n ('We', 'PRP'),\n ('have', 'VBP'),\n ('come', 'VBN'),\n ('to', 'TO'),\n ('dedicate', 'VB'),\n ('a', 'DT'),\n ('portion', 'NN'),\n ('of', 'IN'),\n ('it', 'PRP'),\n (',', ','),\n ('as', 'IN'),\n ('a', 'DT'),\n ('final', 'JJ'),\n ('resting', 'NN'),\n ('place', 'NN'),\n ('for', 'IN'),\n ('those', 'DT'),\n ('who', 'WP'),\n ('died', 'VBD'),\n ('here', 'RB'),\n (',', ','),\n ('that', 'IN'),\n ('the', 'DT'),\n ('nation', 'NN'),\n ('might', 'MD'),\n ('live', 'VB'),\n ('.', '.'),\n ('This', 'DT'),\n ('we', 'PRP'),\n ('may', 'MD'),\n (',', ','),\n ('in', 'IN'),\n ('all', 'DT'),\n ('propriety', 'NN'),\n ('do', 'VBP'),\n ('.', '.'),\n ('But', 'CC'),\n (',', ','),\n ('in', 'IN'),\n ('a', 'DT'),\n ('larger', 'JJR'),\n ('sense', 'NN'),\n (',', ','),\n ('we', 'PRP'),\n ('can', 'MD'),\n ('not', 'RB'),\n ('dedicate', 'VB'),\n ('--', ':'),\n ('we', 'PRP'),\n ('can', 'MD'),\n ('not', 'RB'),\n ('consecrate', 'VB'),\n ('--', ':'),\n ('we', 'PRP'),\n ('can', 'MD'),\n ('not', 'RB'),\n ('hallow', 'VB'),\n (',', ','),\n ('this', 'DT'),\n ('ground', 'NN'),\n ('--', ':'),\n ('The', 'DT'),\n ('brave', 'JJ'),\n ('men', 'NNS'),\n (',', ','),\n ('living', 'NN'),\n ('and', 'CC'),\n ('dead', 'JJ'),\n (',', ','),\n ('who', 'WP'),\n ('struggled', 'VBD'),\n ('here', 'RB'),\n (',', ','),\n ('have', 'VBP'),\n ('hallowed', 'VBN'),\n ('it', 'PRP'),\n (',', ','),\n ('far', 'RB'),\n ('above', 'IN'),\n ('our', 'PRP$'),\n ('poor', 'JJ'),\n ('power', 'NN'),\n ('to', 'TO'),\n ('add', 'VB'),\n ('or', 'CC'),\n ('detract', 'VB'),\n ('.', '.'),\n ('The', 'DT'),\n ('world', 'NN'),\n ('will', 'MD'),\n ('little', 'VB'),\n ('note', 'NN'),\n (',', ','),\n ('nor', 'CC'),\n ('long', 'JJ'),\n ('remember', 'VB'),\n ('what', 'WP'),\n ('we', 'PRP'),\n ('say', 'VBP'),\n ('here', 'RB'),\n (';', ':'),\n ('while', 'IN'),\n ('it', 'PRP'),\n ('can', 'MD'),\n ('never', 'RB'),\n ('forget', 'VB'),\n ('what', 'WP'),\n ('they', 'PRP'),\n ('did', 'VBD'),\n ('here', 'RB'),\n ('.', '.'),\n ('It', 'PRP'),\n ('is', 'VBZ'),\n ('rather', 'RB'),\n ('for', 'IN'),\n ('us', 'PRP'),\n (',', ','),\n ('the', 'DT'),\n ('living', 'NN'),\n (',', ','),\n ('to', 'TO'),\n ('stand', 'VB'),\n ('here', 'RB'),\n (',', ','),\n ('we', 'PRP'),\n ('here', 'RB'),\n ('be', 'VB'),\n ('dedica-ted', 'JJ'),\n ('to', 'TO'),\n ('the', 'DT'),\n ('great', 'JJ'),\n ('task', 'NN'),\n ('remaining', 'VBG'),\n ('before', 'IN'),\n ('us', 'PRP'),\n ('--', ':'),\n ('that', 'IN'),\n (',', ','),\n ('from', 'IN'),\n ('these', 'DT'),\n ('honored', 'VBN'),\n ('dead', 'NN'),\n ('we', 'PRP'),\n ('take', 'VBP'),\n ('increased', 'JJ'),\n ('devotion', 'NN'),\n ('to', 'TO'),\n ('that', 'DT'),\n ('cause', 'NN'),\n ('for', 'IN'),\n ('which', 'WDT'),\n ('they', 'PRP'),\n ('here', 'RB'),\n (',', ','),\n ('gave', 'VBD'),\n ('the', 'DT'),\n ('last', 'JJ'),\n ('full', 'JJ'),\n ('measure', 'NN'),\n ('of', 'IN'),\n ('devotion', 'NN'),\n ('--', ':'),\n ('that', 'IN'),\n ('we', 'PRP'),\n ('here', 'RB'),\n ('highly', 'RB'),\n ('resolve', 'VB'),\n ('these', 'DT'),\n ('dead', 'JJ'),\n ('shall', 'MD'),\n ('not', 'RB'),\n ('have', 'VB'),\n ('died', 'VBN'),\n ('in', 'IN'),\n ('vain', 'NN'),\n (';', ':'),\n ('that', 'IN'),\n ('the', 'DT'),\n ('nation', 'NN'),\n (',', ','),\n ('shall', 'MD'),\n ('have', 'VB'),\n ('a', 'DT'),\n ('new', 'JJ'),\n ('birth', 'NN'),\n ('of', 'IN'),\n ('freedom', 'NN'),\n (',', ','),\n ('and', 'CC'),\n ('that', 'DT'),\n ('government', 'NN'),\n ('of', 'IN'),\n ('the', 'DT'),\n ('people', 'NNS'),\n ('by', 'IN'),\n ('the', 'DT'),\n ('people', 'NNS'),\n ('for', 'IN'),\n ('the', 'DT'),\n ('people', 'NNS'),\n (',', ','),\n ('shall', 'MD'),\n ('not', 'RB'),\n ('perish', 'VB'),\n ('from', 'IN'),\n ('the', 'DT'),\n ('earth', 'NN'),\n ('.', '.')]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-39aa62b58183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# summarize the loaded model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# summarize vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             self.train(\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \"\"\"\n\u001b[1;32m    921\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 922\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1384\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 )\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "# train model\n",
    "model = Word2Vec(word_dict, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitbaseconda74f6bf482b3748dfab216fe046a8ef20",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}