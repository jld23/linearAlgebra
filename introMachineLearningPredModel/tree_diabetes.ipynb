{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook will walk through how to build a decision tree model using `sci-kit learn` for a binary target. The data in question is medical information about a number of adult women of Pima Indian heritage. The goal of the model is to help predict if the woman has diabetes.\n",
    "\n",
    "@misc{Dua:2019 ,\n",
    "author = \"Dua, Dheeru and Graff, Casey\",\n",
    "year = \"2017\",\n",
    "title = \"{UCI} Machine Learning Repository\",\n",
    "url = \"http://archive.ics.uci.edu/ml\",\n",
    "institution = \"University of California, Irvine, School of Information and Computer Sciences\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "These is where all the needed packages are imported for the exercise. If you get an `ModuleNotFoundError` then install the package (pip or conda) before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create a dataframe\n",
    "\n",
    "pandas can read data locally or from a URL. In this case you'll read data from the data directory and create a dataframe named `diabetes` that has health information on 768 women who are over 21 and of Pima Indian heritage.\n",
    "\n",
    "After reading the data you'll use the `shape` method to get a count of the rows and columns. There should be 768 rows and 9 columns.\n",
    "\n",
    "Source: Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('../data/diabetes.csv')\n",
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data for sanity check\n",
    "\n",
    "After reading the data you'll print the first 5 rows using the `head` method to ensure the data appears correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate the inputs from the full data set\n",
    "\n",
    "For modeling in sci-kit learn it is a standard practice to create different objects for the inputs (X varaibles, indepentdent variables) and the target (Y variable, dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first seven columns of data\n",
    "inputs = diabetes.iloc[:, 0:7]\n",
    "# the last column is the target\n",
    "target = diabetes.iloc[:, -1]\n",
    "\n",
    "print(target[45:52], target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and test\n",
    "\n",
    "Creating a `training` and `validation` (some times called a `test`) set help prevent overfitting of the model. A model that is overfit will not be useful in predicting future behavior, which is the point of this modeling in the first place.\n",
    "\n",
    "Below we use the `train_test_split` method to seperate the data into a 70/30 split (70 percent for training and 30 percent for validation). We will use a random number seed so that we get consistent results from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_train, input_test, target_train, target_test = train_test_split(inputs, target, test_size = 0.30, random_state=9878)\n",
    "print(input_train.shape, input_test.shape, target_train.shape, target_test.shape)\n",
    "print(input_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the inputs\n",
    "\n",
    "We need to scale the inputs to improve model performance. There are several scaling methods that can be used. The most popular are MinMaxScaling and Standard Scaling. Scaling needs to be performed for linear methods such as regression and neural networks, it does not need to be done for tree based methods.\n",
    "Standard Scaling the inputs will substract the mean and scale to unit variance. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more information.\n",
    "\n",
    "MinMaxScaling is described [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) \n",
    "\n",
    "You will standardize the training and validation partitions separately to avoid bias and data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(input_train)\n",
    "\n",
    "input_train = scaler.transform(input_train)\n",
    "input_test = scaler.transform(input_test)\n",
    "print(input_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "To train a model, you must create an instance of the method (decision tree in this case) and then use the `fit` method. The way I remember the name is that I'm going to \"fit\" the inputs to the target. \n",
    "\n",
    "In addition to a decision tree we will also create tree ensembles better know as random forest (bagging) and gradient boosting (boosting)\n",
    "\n",
    "Here is where you can experiment with the `hyperparameters` to try and find the best model. For instance the gradient boosting model `gb` is using the default number of 100 trees but `gb2` is using 200 trees to build the model.\n",
    "\n",
    "You can see the documentation details here:\n",
    "* [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "Feel free to experiment and try some new options. You can't hurt anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(input_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(input_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(input_train, target_train)\n",
    "\n",
    "gb2 = GradientBoostingClassifier(n_estimators=200)\n",
    "gb2.fit(input_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "To use the model created by the `fit` method, you must predict values. The code below uses the values from the validation partition to predict if the patient has diabeties. This prediction will then be compared to the actual values and you can assess the efficacy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dtree = dtree.predict(input_test)\n",
    "predictions_rf = rf.predict(input_test)\n",
    "predictions_gb = gb.predict(input_test)\n",
    "predictions_gb2 = gb2.predict(input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Efficacy\n",
    "\n",
    "With predictons complete on the validation partition you can calculate the quality of the model. The confusion matrix, ROC chart, and classification report are a few ways to evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(model_obj):\n",
    "    pred = model_obj.predict(input_test)\n",
    "    print(\"Class: {}\".format(model_obj.__class__))\n",
    "    print(confusion_matrix(target_test,pred))\n",
    "    print(classification_report(target_test,pred))\n",
    "    print(\"Accuracy: {:0.4f}\".format(accuracy_score(target_test,pred)))\n",
    "    plot_roc_curve(model_obj, input_test, target_test);\n",
    "    plot_confusion_matrix(model_obj, input_test, target_test, values_format ='');\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_report(dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through all the models \n",
    "for m in [dtree, rf, gb, gb2]:\n",
    "    model_report(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}