{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook will walk through how to build a regression model using `sci-kit learn` for a binary target. The data in question is medical information about a number of adult women of Pima Indian heritage. The goal of the model is to help predict if the woman has diabetes.\n",
    "\n",
    "@misc{Dua:2019 ,\n",
    "author = \"Dua, Dheeru and Graff, Casey\",\n",
    "year = \"2017\",\n",
    "title = \"{UCI} Machine Learning Repository\",\n",
    "url = \"http://archive.ics.uci.edu/ml\",\n",
    "institution = \"University of California, Irvine, School of Information and Computer Sciences\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "These is where all the needed packages are imported for the exercise. If you get an `ModuleNotFoundError` then install the package (pip or conda) before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create a dataframe\n",
    "\n",
    "pandas can read data locally or from a URL. In this case you'll read data from the data directory and create a dataframe named `diabetes` that has health information on 768 women who are over 21 and of Pima Indian heritage.\n",
    "\n",
    "After reading the data you'll use the `shape` method to get a count of the rows and columns. There should be 768 rows and 9 columns.\n",
    "\n",
    "Source: Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('../data/diabetes.csv')\n",
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data for sanity check\n",
    "\n",
    "After reading the data you'll print the first 5 rows using the `head` method to ensure the data appears correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate the inputs from the full data set\n",
    "\n",
    "For modeling in sci-kit learn it is a standard practice to create different objects for the inputs (X variables, independent variables) and the target (Y variable, dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first seven columns of data\n",
    "inputs = diabetes.iloc[:, 0:7]\n",
    "# the last column is the target\n",
    "target = diabetes.iloc[:, -1]\n",
    "\n",
    "print(target[45:52], target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and test\n",
    "\n",
    "Creating a `training` and `validation` (some times called a `test`) set help prevent overfitting of the model. A model that is overfit will not be useful in predicting future behavior, which is the point of this modeling in the first place.\n",
    "\n",
    "Below we use the `train_test_split` method to seperate the data into a 70/30 split (70 percent for training and 30 percent for validation). We will use a random number seed so that we get consistent results from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "input_train, input_test, target_train, target_test = train_test_split(inputs, target, test_size = 0.30, random_state=9878)\n",
    "print(input_train.shape, input_test.shape, target_train.shape, target_test.shape)\n",
    "print(input_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the inputs\n",
    "\n",
    "We need to scale the inputs to improve model performance. There are several scaling methods that can be used. The most popular are MinMaxScaling and Standard Scaling. Scaling needs to be performed for linear methods such as regression and neural networks, it does not need to be done for tree based methods.\n",
    "Standard Scaling the inputs will substract the mean and scale to unit variance. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more information.\n",
    "\n",
    "MinMaxScaling is described [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) \n",
    "\n",
    "You will standardize the training and validation partitions separately to avoid bias and data leakage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(input_train)\n",
    "\n",
    "input_train = scaler.transform(input_train)\n",
    "input_test = scaler.transform(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(input_train)\n",
    "\n",
    "input_train = scaler.transform(input_train)\n",
    "input_test = scaler.transform(input_test)\n",
    "print(input_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train a model, you must create an instance of the method (regression in this case) and then use the `fit` method. The way I remember the name is that I'm going to \"fit\" the inputs to the target. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Stats Model package\n",
    "\n",
    "Belowe we'll use the `statsmodel` package to fit a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_reg = sm.Logit(target_train, input_train )\n",
    "results = sm_reg.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit a logistic regression\n",
    "\n",
    "Because the target variable is binary, we will use logistic regression to predict if the patient has diabetes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(input_train, target_train)\n",
    "#model_report(log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "To use the model created by the `fit` method, you must predict values. The code below uses the values from the validation partition to predict if the patient has diabeties. This prediction will then be compared to the actual values and you can assess the efficacy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg.predict(input_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Efficacy\n",
    "\n",
    "With predictons complete on the validation partition you can calculate the quality of the model. The confusion matrix, ROC chart, and classification report are a few ways to evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper method to report on the model quality\n",
    "def model_report(model_obj):\n",
    "    pred = model_obj.predict(input_test)\n",
    "    print(\"Class: {}\".format(model_obj.__class__))\n",
    "    print(confusion_matrix(target_test,pred))\n",
    "    print(classification_report(target_test,pred))\n",
    "    print(\"Accuracy: {:0.4f}\".format(accuracy_score(target_test,pred)))\n",
    "    plot_roc_curve(model_obj, input_test, target_test);\n",
    "    plot_confusion_matrix(model_obj, input_test, target_test, values_format ='');\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation component\n",
    "\n",
    "When I evalute the quality of a model I often look at these metrics. Here they are presented individuall then using the other helper function defined above.\n",
    "In the other notebook examples I just use the helper function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(target_test,predictions))\n",
    "print(classification_report(target_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(log_reg, input_test, target_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(log_reg, input_test, target_test, values_format ='');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:0.4f}\".format(accuracy_score(target_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_report(log_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}