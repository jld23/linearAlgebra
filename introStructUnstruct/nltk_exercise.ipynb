{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook I'll illistrate how text analyics can be done using Python and public models\n",
    "\n",
    "**There are a set of exercises to do by hand, followed by code. We love computers because they add faster than we do and never get tiered, but it is essential that you do the hand exercies to fully understand what the computer is doing.**\n",
    "\n",
    "The main work behind this is to take text information and convert it to numerical values so that a computer can perform tasks similar to humans.\n",
    "These tasks include:\n",
    "\n",
    "*  Finding similar words\n",
    "*  Clustering documents\n",
    "*  Natural Language Processing (NLP) \n",
    "    *  Sentiment analysis\n",
    "    *  Language translation\n",
    "    *  Photo captioning\n",
    "\n",
    "NLP is a very broad topic and this is just an introduction for more information start [here](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "Many text models are based on [GloVe](https://nlp.stanford.edu/projects/glove/) and [word2vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "\n",
    "## Word2vec\n",
    "\n",
    "Word2vec was orgionally published in 2013 by Tomas Mikolov and patented while he was working at Google. You can build your own `word2vec` model on any corpus of text but my recommendation is to use a pretrained model. These are usually based on very large collection of text like Newsgroups, quora, or wikipedia.\n",
    "`gensim` is a very popular Python package for using prebuilt language models \n",
    "\n",
    "\n",
    "\n",
    "## GloVe\n",
    "\n",
    "GloVe is a collection of models that were trained on different corpus. The most common was trained on Wikipedia and includes 6 billion tokens and 400k words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that we will follow from [Adam Geitgey](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)\n",
    "\n",
    "\n",
    "![NLP Pipeline](./NLP_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "This step takes a document (sentence in our case) and converts it to a numerical form as well as counts the word frequency.\n",
    "\n",
    "Here is an a paragraph about [Geoffrey Hinton's education](https://en.wikipedia.org/wiki/Geoffrey_Hinton) from Wikipedia:\n",
    "\n",
    "\"Hinton was educated at King's College, Cambridge, graduating in 1970 with a Bachelor of Arts in experimental psychology. He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.\"\n",
    "\n",
    "\n",
    "1. Break the paragraph into sentences.\n",
    "    *  Hinton was educated at King's College, Cambridge, graduating in 1970 with a Bachelor of Arts in experimental psychology.\n",
    "    *  He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.\n",
    "\n",
    "**I'll show you with the first sentence and you need do the second sentence.**\n",
    "\n",
    "1. Take the sentence and break it into word tokens:\n",
    "    \"Hinton\", \"was\", \"educated\", \"at\", \"King's\", \"College\", \"Cambridge\", \"graduating\", \"in\", \"1970\", \"with\", \"a\", \"Bachelor\", \"of\", \"Arts\", \"in\", \"experimental\", \"psychology\", \".\"\n",
    "\n",
    "1. Count the frequency of each word (token)\n",
    "    bag_of_words1 = {'Mason':1, 'likes':1, 'to':1, 'learn':1, 'about':1, 'computers':1}\n",
    "\n",
    "    bag_of_words2 = {}\n",
    "\n",
    "1. Now combine the two sentences **hint:** 'Mason':1 & 'likes':2\n",
    "\n",
    "    `bag_of_words1 + bag_of_words2 = bag_of_words3`\n",
    "\n",
    "    bag_of_words3 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the value (answer() of `bag_of_words3`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Here are the python modules needed to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jadean/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jadean/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jadean/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jadean/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jadean/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/jadean/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "#import gensim\n",
    "#from gensim import corpora\n",
    "#from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The text for this example is the Abraham Lincolns famous [Gettysburg Address](https://en.wikipedia.org/wiki/Gettysburg_Address)\n",
    "\n",
    "The text is written below and assigned to a variable named `gettysburg_address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_address = \"\"\"Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n",
    "\n",
    "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are met on a great battle field of that war. We have come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live. This we may, in all propriety do. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow, this ground-- The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract. The world will little note, nor long remember what we say here; while it can never forget what they did here.\n",
    "\n",
    "It is rather for us, the living, to stand here, we here be dedica-ted to the great task remaining before us -- that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion -- that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people by the people for the people, shall not perish from the earth.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the text into sentences.\n",
    "\n",
    "The first step is to break the text into sentences. Below is the first sentence, please at the rest to the cell. There are a total of 8 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "* Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n",
    "* Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure.\n",
    "* We are met on a great battle field of that war.\n",
    "* We have come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live.\n",
    "* This we may, in all propriety do.\n",
    "* But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow, this ground-- The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract.\n",
    "* The world will little note, nor long remember what we say here; while it can never forget what they did here.\n",
    "* It is rather for us, the living, to stand here, we here be dedicated to the great task remaining before us -- that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion -- that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people by the people for the people, shall not perish from the earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".',\n",
       " 'Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure.',\n",
       " 'We are met on a great battle field of that war.',\n",
       " 'We have come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live.',\n",
       " 'This we may, in all propriety do.',\n",
       " 'But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow, this ground-- The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract.',\n",
       " 'The world will little note, nor long remember what we say here; while it can never forget what they did here.',\n",
       " 'It is rather for us, the living, to stand here, we here be dedica-ted to the great task remaining before us -- that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion -- that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people by the people for the people, shall not perish from the earth.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettysburg_sentences = nltk.sent_tokenize(gettysburg_address)\n",
    "gettysburg_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the text into tokens.\n",
    "\n",
    "Now we need to break each sentence into the individual tokens (words)\n",
    "\n",
    "Here is the first sentence: \n",
    "\n",
    "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth,', 'upon', 'this', 'continent,', 'a', 'new', 'nation,', 'conceived', 'in', 'liberty,', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', '\"', 'all', 'men', 'are', 'created', 'equal', '\"', '.']\n",
    "\n",
    "You will need to do this for sentences 2 & 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "['Now', 'we', 'are', 'engaged', 'in', 'a', 'great', 'civil', 'war,', 'testing', 'whether', 'that', 'nation,', 'or', 'any', 'nation', 'so', 'conceived,', 'and', 'so', 'dedicated,', 'can', 'long', 'endure', '.', 'We', 'are', 'met', 'on', 'a', 'great', 'battle', 'field', 'of', 'that', 'war', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four',\n",
       " 'score',\n",
       " 'and',\n",
       " 'seven',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'our',\n",
       " 'fathers',\n",
       " 'brought',\n",
       " 'forth',\n",
       " ',',\n",
       " 'upon',\n",
       " 'this',\n",
       " 'continent',\n",
       " ',']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettysburg_word_tokens = nltk.tokenize.word_tokenize(gettysburg_address)\n",
    "\n",
    "# Show the first 15 words\n",
    "gettysburg_word_tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "To efficiently use NLP knowing the part of speech is important. This might seem like going back to grammar school and diagramming sentences because it is :)\n",
    "\n",
    "Here are the parts of speech that `NLTK` identifies:\n",
    "```\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‘big’\n",
    "JJR adjective, comparative ‘bigger’\n",
    "JJS adjective, superlative ‘biggest’\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‘desk’\n",
    "NNS noun plural ‘desks’\n",
    "NNP proper noun, singular ‘Harrison’\n",
    "NNPS proper noun, plural ‘Americans’\n",
    "PDT predeterminer ‘all the kids’\n",
    "POS possessive ending parent’s\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO, to go ‘to’ the store.\n",
    "UH interjection, errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense, took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle is taken\n",
    "VBP verb, sing. present, known-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-adverb where, when\n",
    "```\n",
    "\n",
    "Here is the solution for the first sentence:\n",
    "```\n",
    "[('Four', 'CD'),\n",
    " ('score', 'NN'),\n",
    " ('and', 'CC'),\n",
    " ('seven', 'CD'),\n",
    " ('years', 'NNS'),\n",
    " ('ago', 'RB'),\n",
    " ('our', 'PRP$'),\n",
    " ('fathers', 'NNS'),\n",
    " ('brought', 'VBD'),\n",
    " ('forth', 'NN'),\n",
    " (',', ','),\n",
    " ('upon', 'IN'),\n",
    " ('this', 'DT'),\n",
    " ('continent', 'NN'),\n",
    " (',', ','),\n",
    " ('a', 'DT'),\n",
    " ('new', 'JJ'),\n",
    " ('nation', 'NN'),\n",
    " (',', ','),\n",
    " ('conceived', 'VBN'),\n",
    " ('in', 'IN'),\n",
    " ('liberty', 'NN'),\n",
    " (',', ','),\n",
    " ('and', 'CC'),\n",
    " ('dedicated', 'VBD'),\n",
    " ('to', 'TO'),\n",
    " ('the', 'DT'),\n",
    " ('proposition', 'NN'),\n",
    " ('that', 'IN'),\n",
    " ('``', '``'),\n",
    " ('all', 'DT'),\n",
    " ('men', 'NNS'),\n",
    " ('are', 'VBP'),\n",
    " ('created', 'VBN'),\n",
    " ('equal', 'JJ'),\n",
    " (\"''\", \"''\")]\n",
    "```\n",
    "\n",
    "Take a few minutes to identify the parts of speech for sentences 2 & 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Now', 'RB'),\n",
       " ('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('engaged', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('civil', 'JJ'),\n",
       " ('war', 'NN'),\n",
       " (',', ','),\n",
       " ('testing', 'VBG'),\n",
       " ('whether', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('nation', 'NN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('any', 'DT'),\n",
       " ('nation', 'NN'),\n",
       " ('so', 'RB'),\n",
       " ('conceived', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('so', 'RB'),\n",
       " ('dedicated', 'JJ'),\n",
       " (',', ','),\n",
       " ('can', 'MD'),\n",
       " ('long', 'VB'),\n",
       " ('endure', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('met', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('battle', 'NN'),\n",
       " ('field', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('war', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))[37:78]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "In order to improve the quality of search and clustering, words are stemmed to their root. Stemming makes children and child the same since the only difference is the quantity. \n",
    "\n",
    "Here are the stems for the first sentence:\n",
    "```\n",
    "Four  :  four\n",
    "score  :  score\n",
    "and  :  and\n",
    "seven  :  seven\n",
    "years  :  year\n",
    "ago  :  ago\n",
    "our  :  our\n",
    "fathers  :  father\n",
    "brought  :  brought\n",
    "forth  :  forth\n",
    ",  :  ,\n",
    "upon  :  upon\n",
    "this  :  thi\n",
    "continent  :  contin\n",
    ",  :  ,\n",
    "a  :  a\n",
    "new  :  new\n",
    "nation  :  nation\n",
    ",  :  ,\n",
    "conceived  :  conceiv\n",
    "in  :  in\n",
    "liberty  :  liberti\n",
    ",  :  ,\n",
    "and  :  and\n",
    "dedicated  :  dedic\n",
    "to  :  to\n",
    "the  :  the\n",
    "proposition  :  proposit\n",
    "that  :  that\n",
    "``  :  ``\n",
    "all  :  all\n",
    "men  :  men\n",
    "are  :  are\n",
    "created  :  creat\n",
    "equal  :  equal\n",
    "''  :  ''\n",
    "```\n",
    "\n",
    "Stem the words for sentences 2 & 3. Some of the stems are shorter than you might expect (created => creat), just do your best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now  :  now\n",
      "we  :  we\n",
      "are  :  are\n",
      "engaged  :  engag\n",
      "in  :  in\n",
      "a  :  a\n",
      "great  :  great\n",
      "civil  :  civil\n",
      "war  :  war\n",
      ",  :  ,\n",
      "testing  :  test\n",
      "whether  :  whether\n",
      "that  :  that\n",
      "nation  :  nation\n",
      ",  :  ,\n",
      "or  :  or\n",
      "any  :  ani\n",
      "nation  :  nation\n",
      "so  :  so\n",
      "conceived  :  conceiv\n",
      ",  :  ,\n",
      "and  :  and\n",
      "so  :  so\n",
      "dedicated  :  dedic\n",
      ",  :  ,\n",
      "can  :  can\n",
      "long  :  long\n",
      "endure  :  endur\n",
      ".  :  .\n",
      "We  :  We\n",
      "are  :  are\n",
      "met  :  met\n",
      "on  :  on\n",
      "a  :  a\n",
      "great  :  great\n",
      "battle  :  battl\n",
      "field  :  field\n",
      "of  :  of\n",
      "that  :  that\n",
      "war  :  war\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "ps = PorterStemmer() \n",
    "   \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[37:78]: \n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ps = PorterStemmer() \n",
    "   \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[:36]: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Stemming and Lemmatization appear very similar and for many words they are identical. Lemmatization is preferred over stemming because takes into account other items like part of speech in addition to just stemming (see [Morphology](https://en.wikipedia.org/wiki/Morphology_(linguistics)) ).\n",
    "\n",
    "Run the code below to see the lemmatization of sentences 2 & 3 then compare them to your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now  :  Now\n",
      "we  :  we\n",
      "are  :  are\n",
      "engaged  :  engaged\n",
      "in  :  in\n",
      "a  :  a\n",
      "great  :  great\n",
      "civil  :  civil\n",
      "war  :  war\n",
      ",  :  ,\n",
      "testing  :  testing\n",
      "whether  :  whether\n",
      "that  :  that\n",
      "nation  :  nation\n",
      ",  :  ,\n",
      "or  :  or\n",
      "any  :  any\n",
      "nation  :  nation\n",
      "so  :  so\n",
      "conceived  :  conceived\n",
      ",  :  ,\n",
      "and  :  and\n",
      "so  :  so\n",
      "dedicated  :  dedicated\n",
      ",  :  ,\n",
      "can  :  can\n",
      "long  :  long\n",
      "endure  :  endure\n",
      ".  :  .\n",
      "We  :  We\n",
      "are  :  are\n",
      "met  :  met\n",
      "on  :  on\n",
      "a  :  a\n",
      "great  :  great\n",
      "battle  :  battle\n",
      "field  :  field\n",
      "of  :  of\n",
      "that  :  that\n",
      "war  :  war\n",
      ".  :  .\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[37:78]: \n",
    "    print(w, \" : \", lemmatizer.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[:36]: \n",
    "    print(w, \" : \", lemmatizer.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words\n",
    "\n",
    "Stop words are those that \n",
    "\n",
    "Below is the first sentence with stop words removed. The sentence was 36 tokens originally with the stop words removed it is 25.\n",
    "\n",
    "```\n",
    "['Four',\n",
    " 'score',\n",
    " 'seven',\n",
    " 'years',\n",
    " 'ago',\n",
    " 'fathers',\n",
    " 'brought',\n",
    " 'forth',\n",
    " ',',\n",
    " 'upon',\n",
    " 'continent',\n",
    " ',',\n",
    " 'new',\n",
    " 'nation',\n",
    " ',',\n",
    " 'conceived',\n",
    " 'liberty',\n",
    " ',',\n",
    " 'dedicated',\n",
    " 'proposition',\n",
    " '``',\n",
    " 'men',\n",
    " 'created',\n",
    " 'equal',\n",
    " \"''\"]\n",
    "```\n",
    "\n",
    "To see the list of stop words in `nltk`, run the code below (there are 179). Then remove any word that is in sentences 2 & 3 AND in the stop word list (the list is alphabetic). \n",
    "\n",
    "The correct answer has 25 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four',\n",
       " 'score',\n",
       " 'seven',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'fathers',\n",
       " 'brought',\n",
       " 'forth',\n",
       " ',',\n",
       " 'upon',\n",
       " 'continent',\n",
       " ',',\n",
       " 'new',\n",
       " 'nation',\n",
       " ',',\n",
       " 'conceived',\n",
       " 'liberty',\n",
       " ',',\n",
       " 'dedicated',\n",
       " 'proposition',\n",
       " '``',\n",
       " 'men',\n",
       " 'created',\n",
       " 'equal',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_removed = [w for w in nltk.tokenize.word_tokenize(gettysburg_address)[:36] if not w in stop_words] \n",
    "stop_words_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now',\n",
       " 'engaged',\n",
       " 'great',\n",
       " 'civil',\n",
       " 'war',\n",
       " ',',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'nation',\n",
       " ',',\n",
       " 'nation',\n",
       " 'conceived',\n",
       " ',',\n",
       " 'dedicated',\n",
       " ',',\n",
       " 'long',\n",
       " 'endure',\n",
       " '.',\n",
       " 'We',\n",
       " 'met',\n",
       " 'great',\n",
       " 'battle',\n",
       " 'field',\n",
       " 'war',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "stop_words_removed = [w for w in nltk.tokenize.word_tokenize(gettysburg_address)[37:78] if not w in stop_words] \n",
    "len(stop_words_removed)\n",
    "stop_words_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Four', 'CD', 'O'),\n",
      " ('score', 'NN', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('seven', 'CD', 'O'),\n",
      " ('years', 'NNS', 'O'),\n",
      " ('ago', 'RB', 'O'),\n",
      " ('our', 'PRP$', 'O'),\n",
      " ('fathers', 'NNS', 'O'),\n",
      " ('brought', 'VBD', 'O'),\n",
      " ('forth', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('upon', 'IN', 'O'),\n",
      " ('this', 'DT', 'O'),\n",
      " ('continent', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('a', 'DT', 'O'),\n",
      " ('new', 'JJ', 'O'),\n",
      " ('nation', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('conceived', 'VBN', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('liberty', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('dedicated', 'VBD', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('proposition', 'NN', 'O'),\n",
      " ('that', 'IN', 'O'),\n",
      " ('``', '``', 'O'),\n",
      " ('all', 'DT', 'O'),\n",
      " ('men', 'NNS', 'O'),\n",
      " ('are', 'VBP', 'O'),\n",
      " ('created', 'VBN', 'O'),\n",
      " ('equal', 'JJ', 'O'),\n",
      " (\"''\", \"''\", 'O')]\n"
     ]
    }
   ],
   "source": [
    "ne_tree = nltk.ne_chunk(nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))[:36])\n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Four/CD\n",
      "  score/NN\n",
      "  and/CC\n",
      "  seven/CD\n",
      "  years/NNS\n",
      "  ago/RB\n",
      "  our/PRP$\n",
      "  fathers/NNS\n",
      "  brought/VBD\n",
      "  forth/NN\n",
      "  ,/,\n",
      "  upon/IN\n",
      "  this/DT\n",
      "  continent/NN\n",
      "  ,/,\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  nation/NN\n",
      "  ,/,\n",
      "  conceived/VBN\n",
      "  in/IN\n",
      "  liberty/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  dedicated/VBD\n",
      "  to/TO\n",
      "  the/DT\n",
      "  proposition/NN\n",
      "  that/IN\n",
      "  ``/``\n",
      "  all/DT\n",
      "  men/NNS\n",
      "  are/VBP\n",
      "  created/VBN\n",
      "  equal/JJ\n",
      "  ''/'')\n"
     ]
    }
   ],
   "source": [
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Four', 'CD', 'O'),\n",
      " ('score', 'NN', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('seven', 'CD', 'O'),\n",
      " ('years', 'NNS', 'O'),\n",
      " ('ago', 'RB', 'O'),\n",
      " ('our', 'PRP$', 'O'),\n",
      " ('fathers', 'NNS', 'O'),\n",
      " ('brought', 'VBD', 'O'),\n",
      " ('forth', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('upon', 'IN', 'O'),\n",
      " ('this', 'DT', 'O'),\n",
      " ('continent', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('a', 'DT', 'O'),\n",
      " ('new', 'JJ', 'O'),\n",
      " ('nation', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('conceived', 'VBN', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('liberty', 'NN', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('dedicated', 'VBD', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('proposition', 'NN', 'O'),\n",
      " ('that', 'IN', 'O'),\n",
      " ('``', '``', 'O'),\n",
      " ('all', 'DT', 'O'),\n",
      " ('men', 'NNS', 'O'),\n",
      " ('are', 'VBP', 'O'),\n",
      " ('created', 'VBN', 'O'),\n",
      " ('equal', 'JJ', 'O'),\n",
      " (\"''\", \"''\", 'O')]\n"
     ]
    }
   ],
   "source": [
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular package, in addition to `nltk`, is `spacy`. The render display of named entity recognition is much better in my opinion but it only found two named entities using default options.\n",
    "\n",
    "Here is the screen shot of the analysis:\n",
    "\n",
    "![spacy](./spacy_gburg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(37 unique tokens: ['1970', '1978', 'Arts', 'Bachelor', 'Cambridge,']...)\n"
     ]
    }
   ],
   "source": [
    "paragraph = [\"Hinton was educated at King's College, Cambridge, graduating in 1970 with a Bachelor of Arts in experimental psychology. He continued his study at the University of Edinburgh where he was awarded a PhD in artificial intelligence in 1978 for research supervised by Christopher Longuet-Higgins.\"]\n",
    "\n",
    "texts = [[text for text in doc.split()] for doc in paragraph]\n",
    "word_dict = corpora.Dictionary(texts)\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the unique id's for each token (word) in the text.\n",
    "\n",
    "You can see that is sorted alphabetically. Every unique word is in here -- including 'He' and 'he'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1970': 0, '1978': 1, 'Arts': 2, 'Bachelor': 3, 'Cambridge,': 4, 'Christopher': 5, 'College,': 6, 'Edinburgh': 7, 'He': 8, 'Hinton': 9, \"King's\": 10, 'Longuet-Higgins.': 11, 'PhD': 12, 'University': 13, 'a': 14, 'artificial': 15, 'at': 16, 'awarded': 17, 'by': 18, 'continued': 19, 'educated': 20, 'experimental': 21, 'for': 22, 'graduating': 23, 'he': 24, 'his': 25, 'in': 26, 'intelligence': 27, 'of': 28, 'psychology.': 29, 'research': 30, 'study': 31, 'supervised': 32, 'the': 33, 'was': 34, 'where': 35, 'with': 36}\n"
     ]
    }
   ],
   "source": [
    "print(word_dict.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a pretrained word2vec model on the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_address = \"\"\"Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n",
    "\n",
    "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are met on a great battle field of that war. We have come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live. This we may, in all propriety do. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow, this ground-- The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract. The world will little note, nor long remember what we say here; while it can never forget what they did here.\n",
    "\n",
    "It is rather for us, the living, to stand here, we here be dedica-ted to the great task remaining before us -- that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion -- that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people by the people for the people, shall not perish from the earth.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the speech into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_sentences = nltk.sent_tokenize(gettysburg_address)\n",
    "gettysburg_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the speech into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four',\n",
       " 'score',\n",
       " 'and',\n",
       " 'seven',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'our',\n",
       " 'fathers',\n",
       " 'brought',\n",
       " 'forth',\n",
       " ',',\n",
       " 'upon',\n",
       " 'this',\n",
       " 'continent',\n",
       " ',',\n",
       " 'a',\n",
       " 'new',\n",
       " 'nation',\n",
       " ',',\n",
       " 'conceived',\n",
       " 'in',\n",
       " 'liberty',\n",
       " ',',\n",
       " 'and',\n",
       " 'dedicated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'proposition',\n",
       " 'that',\n",
       " '``',\n",
       " 'all',\n",
       " 'men',\n",
       " 'are',\n",
       " 'created',\n",
       " 'equal',\n",
       " \"''\",\n",
       " '.',\n",
       " 'Now',\n",
       " 'we',\n",
       " 'are',\n",
       " 'engaged',\n",
       " 'in',\n",
       " 'a',\n",
       " 'great',\n",
       " 'civil',\n",
       " 'war',\n",
       " ',',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'that',\n",
       " 'nation',\n",
       " ',',\n",
       " 'or',\n",
       " 'any',\n",
       " 'nation',\n",
       " 'so',\n",
       " 'conceived',\n",
       " ',',\n",
       " 'and',\n",
       " 'so',\n",
       " 'dedicated',\n",
       " ',',\n",
       " 'can',\n",
       " 'long',\n",
       " 'endure',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'met',\n",
       " 'on',\n",
       " 'a',\n",
       " 'great',\n",
       " 'battle',\n",
       " 'field',\n",
       " 'of',\n",
       " 'that',\n",
       " 'war',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'come',\n",
       " 'to',\n",
       " 'dedicate',\n",
       " 'a',\n",
       " 'portion',\n",
       " 'of',\n",
       " 'it',\n",
       " ',',\n",
       " 'as',\n",
       " 'a',\n",
       " 'final',\n",
       " 'resting',\n",
       " 'place',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'died',\n",
       " 'here',\n",
       " ',',\n",
       " 'that',\n",
       " 'the',\n",
       " 'nation',\n",
       " 'might',\n",
       " 'live',\n",
       " '.',\n",
       " 'This',\n",
       " 'we',\n",
       " 'may',\n",
       " ',',\n",
       " 'in',\n",
       " 'all',\n",
       " 'propriety',\n",
       " 'do',\n",
       " '.',\n",
       " 'But',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'sense',\n",
       " ',',\n",
       " 'we',\n",
       " 'can',\n",
       " 'not',\n",
       " 'dedicate',\n",
       " '--',\n",
       " 'we',\n",
       " 'can',\n",
       " 'not',\n",
       " 'consecrate',\n",
       " '--',\n",
       " 'we',\n",
       " 'can',\n",
       " 'not',\n",
       " 'hallow',\n",
       " ',',\n",
       " 'this',\n",
       " 'ground',\n",
       " '--',\n",
       " 'The',\n",
       " 'brave',\n",
       " 'men',\n",
       " ',',\n",
       " 'living',\n",
       " 'and',\n",
       " 'dead',\n",
       " ',',\n",
       " 'who',\n",
       " 'struggled',\n",
       " 'here',\n",
       " ',',\n",
       " 'have',\n",
       " 'hallowed',\n",
       " 'it',\n",
       " ',',\n",
       " 'far',\n",
       " 'above',\n",
       " 'our',\n",
       " 'poor',\n",
       " 'power',\n",
       " 'to',\n",
       " 'add',\n",
       " 'or',\n",
       " 'detract',\n",
       " '.',\n",
       " 'The',\n",
       " 'world',\n",
       " 'will',\n",
       " 'little',\n",
       " 'note',\n",
       " ',',\n",
       " 'nor',\n",
       " 'long',\n",
       " 'remember',\n",
       " 'what',\n",
       " 'we',\n",
       " 'say',\n",
       " 'here',\n",
       " ';',\n",
       " 'while',\n",
       " 'it',\n",
       " 'can',\n",
       " 'never',\n",
       " 'forget',\n",
       " 'what',\n",
       " 'they',\n",
       " 'did',\n",
       " 'here',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'rather',\n",
       " 'for',\n",
       " 'us',\n",
       " ',',\n",
       " 'the',\n",
       " 'living',\n",
       " ',',\n",
       " 'to',\n",
       " 'stand',\n",
       " 'here',\n",
       " ',',\n",
       " 'we',\n",
       " 'here',\n",
       " 'be',\n",
       " 'dedica-ted',\n",
       " 'to',\n",
       " 'the',\n",
       " 'great',\n",
       " 'task',\n",
       " 'remaining',\n",
       " 'before',\n",
       " 'us',\n",
       " '--',\n",
       " 'that',\n",
       " ',',\n",
       " 'from',\n",
       " 'these',\n",
       " 'honored',\n",
       " 'dead',\n",
       " 'we',\n",
       " 'take',\n",
       " 'increased',\n",
       " 'devotion',\n",
       " 'to',\n",
       " 'that',\n",
       " 'cause',\n",
       " 'for',\n",
       " 'which',\n",
       " 'they',\n",
       " 'here',\n",
       " ',',\n",
       " 'gave',\n",
       " 'the',\n",
       " 'last',\n",
       " 'full',\n",
       " 'measure',\n",
       " 'of',\n",
       " 'devotion',\n",
       " '--',\n",
       " 'that',\n",
       " 'we',\n",
       " 'here',\n",
       " 'highly',\n",
       " 'resolve',\n",
       " 'these',\n",
       " 'dead',\n",
       " 'shall',\n",
       " 'not',\n",
       " 'have',\n",
       " 'died',\n",
       " 'in',\n",
       " 'vain',\n",
       " ';',\n",
       " 'that',\n",
       " 'the',\n",
       " 'nation',\n",
       " ',',\n",
       " 'shall',\n",
       " 'have',\n",
       " 'a',\n",
       " 'new',\n",
       " 'birth',\n",
       " 'of',\n",
       " 'freedom',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " 'government',\n",
       " 'of',\n",
       " 'the',\n",
       " 'people',\n",
       " 'by',\n",
       " 'the',\n",
       " 'people',\n",
       " 'for',\n",
       " 'the',\n",
       " 'people',\n",
       " ',',\n",
       " 'shall',\n",
       " 'not',\n",
       " 'perish',\n",
       " 'from',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettysburg_word_tokens = nltk.tokenize.word_tokenize(gettysburg_address)\n",
    "gettysburg_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Four', 'CD'),\n",
       " ('score', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('seven', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('our', 'PRP$'),\n",
       " ('fathers', 'NNS'),\n",
       " ('brought', 'VBD'),\n",
       " ('forth', 'NN'),\n",
       " (',', ','),\n",
       " ('upon', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('continent', 'NN'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('nation', 'NN'),\n",
       " (',', ','),\n",
       " ('conceived', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('liberty', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('dedicated', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('proposition', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('``', '``'),\n",
       " ('all', 'DT'),\n",
       " ('men', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('created', 'VBN'),\n",
       " ('equal', 'JJ'),\n",
       " (\"''\", \"''\"),\n",
       " ('.', '.'),\n",
       " ('Now', 'RB'),\n",
       " ('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('engaged', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('civil', 'JJ'),\n",
       " ('war', 'NN'),\n",
       " (',', ','),\n",
       " ('testing', 'VBG'),\n",
       " ('whether', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('nation', 'NN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('any', 'DT'),\n",
       " ('nation', 'NN'),\n",
       " ('so', 'RB'),\n",
       " ('conceived', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('so', 'RB'),\n",
       " ('dedicated', 'JJ'),\n",
       " (',', ','),\n",
       " ('can', 'MD'),\n",
       " ('long', 'VB'),\n",
       " ('endure', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('met', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('battle', 'NN'),\n",
       " ('field', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('war', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('come', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('dedicate', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('portion', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " (',', ','),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('final', 'JJ'),\n",
       " ('resting', 'NN'),\n",
       " ('place', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('who', 'WP'),\n",
       " ('died', 'VBD'),\n",
       " ('here', 'RB'),\n",
       " (',', ','),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('nation', 'NN'),\n",
       " ('might', 'MD'),\n",
       " ('live', 'VB'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('we', 'PRP'),\n",
       " ('may', 'MD'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('propriety', 'NN'),\n",
       " ('do', 'VBP'),\n",
       " ('.', '.'),\n",
       " ('But', 'CC'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('larger', 'JJR'),\n",
       " ('sense', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('dedicate', 'VB'),\n",
       " ('--', ':'),\n",
       " ('we', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('consecrate', 'VB'),\n",
       " ('--', ':'),\n",
       " ('we', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('hallow', 'VB'),\n",
       " (',', ','),\n",
       " ('this', 'DT'),\n",
       " ('ground', 'NN'),\n",
       " ('--', ':'),\n",
       " ('The', 'DT'),\n",
       " ('brave', 'JJ'),\n",
       " ('men', 'NNS'),\n",
       " (',', ','),\n",
       " ('living', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('dead', 'JJ'),\n",
       " (',', ','),\n",
       " ('who', 'WP'),\n",
       " ('struggled', 'VBD'),\n",
       " ('here', 'RB'),\n",
       " (',', ','),\n",
       " ('have', 'VBP'),\n",
       " ('hallowed', 'VBN'),\n",
       " ('it', 'PRP'),\n",
       " (',', ','),\n",
       " ('far', 'RB'),\n",
       " ('above', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('poor', 'JJ'),\n",
       " ('power', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('add', 'VB'),\n",
       " ('or', 'CC'),\n",
       " ('detract', 'VB'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('little', 'VB'),\n",
       " ('note', 'NN'),\n",
       " (',', ','),\n",
       " ('nor', 'CC'),\n",
       " ('long', 'JJ'),\n",
       " ('remember', 'VB'),\n",
       " ('what', 'WP'),\n",
       " ('we', 'PRP'),\n",
       " ('say', 'VBP'),\n",
       " ('here', 'RB'),\n",
       " (';', ':'),\n",
       " ('while', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('never', 'RB'),\n",
       " ('forget', 'VB'),\n",
       " ('what', 'WP'),\n",
       " ('they', 'PRP'),\n",
       " ('did', 'VBD'),\n",
       " ('here', 'RB'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('rather', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('living', 'NN'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('stand', 'VB'),\n",
       " ('here', 'RB'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('here', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('dedica-ted', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('task', 'NN'),\n",
       " ('remaining', 'VBG'),\n",
       " ('before', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('--', ':'),\n",
       " ('that', 'IN'),\n",
       " (',', ','),\n",
       " ('from', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('honored', 'VBN'),\n",
       " ('dead', 'NN'),\n",
       " ('we', 'PRP'),\n",
       " ('take', 'VBP'),\n",
       " ('increased', 'JJ'),\n",
       " ('devotion', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('that', 'DT'),\n",
       " ('cause', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('they', 'PRP'),\n",
       " ('here', 'RB'),\n",
       " (',', ','),\n",
       " ('gave', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('last', 'JJ'),\n",
       " ('full', 'JJ'),\n",
       " ('measure', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('devotion', 'NN'),\n",
       " ('--', ':'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('here', 'RB'),\n",
       " ('highly', 'RB'),\n",
       " ('resolve', 'VB'),\n",
       " ('these', 'DT'),\n",
       " ('dead', 'JJ'),\n",
       " ('shall', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('have', 'VB'),\n",
       " ('died', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('vain', 'NN'),\n",
       " (';', ':'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('nation', 'NN'),\n",
       " (',', ','),\n",
       " ('shall', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('birth', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('freedom', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('that', 'DT'),\n",
       " ('government', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " (',', ','),\n",
       " ('shall', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('perish', 'VB'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('earth', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-39aa62b58183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# summarize the loaded model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# summarize vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             self.train(\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \"\"\"\n\u001b[1;32m    921\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 922\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1384\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 )\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "# train model\n",
    "model = Word2Vec(word_dict, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python37064bitbaseconda74f6bf482b3748dfab216fe046a8ef20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
