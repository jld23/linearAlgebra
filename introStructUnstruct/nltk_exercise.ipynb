{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook I'll illustrate how text analytics can be done using Python and public models\n",
    "\n",
    "**There are a set of exercises to do by hand, followed by code. We love computers because they add faster than we do and never get tired, but it is essential that you do the hand exercises to fully understand what the computer is doing.**\n",
    "\n",
    "The main work behind this is to take text information and convert it to numerical values so that a computer can perform tasks similar to humans.\n",
    "These tasks include:\n",
    "\n",
    "*  Finding similar words\n",
    "*  Clustering documents\n",
    "*  Natural Language Processing (NLP) \n",
    "    *  Sentiment analysis\n",
    "    *  Language translation\n",
    "    *  Photo captioning\n",
    "\n",
    "NLP is a very broad topic and this is just an introduction for more information start [here](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "Many text models are based on [GloVe](https://nlp.stanford.edu/projects/glove/) and [word2vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "\n",
    "## Word2vec\n",
    "\n",
    "Word2vec was originally published in 2013 by Tomas Mikolov and patented while he was working at Google. You can build your own `word2vec` model on any corpus of text but my recommendation is to use a pre-trained model. These are usually based on very large collection of text like Newsgroups, Quora, or Wikipedia.\n",
    "`gensim` is a very popular Python package for using prebuilt language models \n",
    "\n",
    "\n",
    "\n",
    "## GloVe\n",
    "\n",
    "GloVe is a collection of models that were trained on different corpus. The most common was trained on Wikipedia and includes 6 billion tokens and 400k words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram that we will follow from [Adam Geitgey](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)\n",
    "\n",
    "\n",
    "![NLP Pipeline](./NLP_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Here are the python modules needed to run this code\n",
    "\n",
    "This example will use the `nltk` package for the text tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The text for this example is Abraham Lincoln's famous [Gettysburg Address](https://en.wikipedia.org/wiki/Gettysburg_Address)\n",
    "\n",
    "The text is written below and assigned to a variable named `gettysburg_address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_address = \"\"\"Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n",
    "\n",
    "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are met on a great battle field of that war. We have come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live. This we may, in all propriety do. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow, this ground-- The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract. The world will little note, nor long remember what we say here; while it can never forget what they did here.\n",
    "\n",
    "It is rather for us, the living, to stand here, we here be dedica-ted to the great task remaining before us -- that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion -- that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people by the people for the people, shall not perish from the earth.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break the text into sentences.\n",
    "\n",
    "The first step is to break the text into sentences. Below is the first sentence, please add the rest to the cell. There are a total of 8 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that \"all men are created equal\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_sentences = nltk.sent_tokenize(gettysburg_address)\n",
    "gettysburg_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break the text into tokens.\n",
    "\n",
    "Now we need to break each sentence into the individual tokens (words)\n",
    "\n",
    "Here is the first sentence: \n",
    "\n",
    "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth,', 'upon', 'this', 'continent,', 'a', 'new', 'nation,', 'conceived', 'in', 'liberty,', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', '\"', 'all', 'men', 'are', 'created', 'equal', '\"', '.']\n",
    "\n",
    "You will need to do this for sentences 2 & 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg_word_tokens = nltk.tokenize.word_tokenize(gettysburg_address)\n",
    "\n",
    "# Show the first 15 words\n",
    "gettysburg_word_tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "To efficiently use NLP knowing the part of speech is important. This might seem like going back to grammar school and diagramming sentences because it is :)\n",
    "\n",
    "Here are the parts of speech that `NLTK` identifies:\n",
    "```\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‘big’\n",
    "JJR adjective, comparative ‘bigger’\n",
    "JJS adjective, superlative ‘biggest’\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‘desk’\n",
    "NNS noun plural ‘desks’\n",
    "NNP proper noun, singular ‘Harrison’\n",
    "NNPS proper noun, plural ‘Americans’\n",
    "PDT predeterminer ‘all the kids’\n",
    "POS possessive ending parent’s\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO, to go ‘to’ the store.\n",
    "UH interjection, errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense, took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle is taken\n",
    "VBP verb, sing. present, known-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-adverb where, when\n",
    "```\n",
    "\n",
    "Here is the solution for the first sentence:\n",
    "```\n",
    "[('Four', 'CD'),\n",
    " ('score', 'NN'),\n",
    " ('and', 'CC'),\n",
    " ('seven', 'CD'),\n",
    " ('years', 'NNS'),\n",
    " ('ago', 'RB'),\n",
    " ('our', 'PRP$'),\n",
    " ('fathers', 'NNS'),\n",
    " ('brought', 'VBD'),\n",
    " ('forth', 'NN'),\n",
    " (',', ','),\n",
    " ('upon', 'IN'),\n",
    " ('this', 'DT'),\n",
    " ('continent', 'NN'),\n",
    " (',', ','),\n",
    " ('a', 'DT'),\n",
    " ('new', 'JJ'),\n",
    " ('nation', 'NN'),\n",
    " (',', ','),\n",
    " ('conceived', 'VBN'),\n",
    " ('in', 'IN'),\n",
    " ('liberty', 'NN'),\n",
    " (',', ','),\n",
    " ('and', 'CC'),\n",
    " ('dedicated', 'VBD'),\n",
    " ('to', 'TO'),\n",
    " ('the', 'DT'),\n",
    " ('proposition', 'NN'),\n",
    " ('that', 'IN'),\n",
    " ('``', '``'),\n",
    " ('all', 'DT'),\n",
    " ('men', 'NNS'),\n",
    " ('are', 'VBP'),\n",
    " ('created', 'VBN'),\n",
    " ('equal', 'JJ'),\n",
    " (\"''\", \"''\")]\n",
    "```\n",
    "\n",
    "Take a few minutes to identify the parts of speech for sentences 2 & 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "In order to improve the quality of search and clustering, words are stemmed to their root. Stemming makes children and child the same since the only difference is the quantity. \n",
    "\n",
    "Here are the stems for the first sentence:\n",
    "```\n",
    "Four  :  four\n",
    "score  :  score\n",
    "and  :  and\n",
    "seven  :  seven\n",
    "years  :  year\n",
    "ago  :  ago\n",
    "our  :  our\n",
    "fathers  :  father\n",
    "brought  :  brought\n",
    "forth  :  forth\n",
    ",  :  ,\n",
    "upon  :  upon\n",
    "this  :  thi\n",
    "continent  :  contin\n",
    ",  :  ,\n",
    "a  :  a\n",
    "new  :  new\n",
    "nation  :  nation\n",
    ",  :  ,\n",
    "conceived  :  conceiv\n",
    "in  :  in\n",
    "liberty  :  liberti\n",
    ",  :  ,\n",
    "and  :  and\n",
    "dedicated  :  dedic\n",
    "to  :  to\n",
    "the  :  the\n",
    "proposition  :  proposit\n",
    "that  :  that\n",
    "``  :  ``\n",
    "all  :  all\n",
    "men  :  men\n",
    "are  :  are\n",
    "created  :  creat\n",
    "equal  :  equal\n",
    "''  :  ''\n",
    "```\n",
    "\n",
    "Stem the words for sentences 2 & 3. Some of the stems are shorter than you might expect (created => creat), just do your best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() \n",
    "   \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[:36]: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Stemming and Lemmatization appear very similar and for many words they are identical. Lemmatization is preferred over stemming because takes into account other items like part of speech in addition to just stemming (see [Morphology](https://en.wikipedia.org/wiki/Morphology_(linguistics)) ).\n",
    "\n",
    "Run the code below to see the lemmatization of sentences 2 & 3 then compare them to your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[37:78]: \n",
    "    print(w, \" : \", lemmatizer.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "for w in nltk.tokenize.word_tokenize(gettysburg_address)[:36]: \n",
    "    print(w, \" : \", lemmatizer.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words\n",
    "\n",
    "Stop words are those that \n",
    "\n",
    "Below is the first sentence with stop words removed. The sentence was 36 tokens originally with the stop words removed it is 25.\n",
    "\n",
    "```\n",
    "['Four',\n",
    " 'score',\n",
    " 'seven',\n",
    " 'years',\n",
    " 'ago',\n",
    " 'fathers',\n",
    " 'brought',\n",
    " 'forth',\n",
    " ',',\n",
    " 'upon',\n",
    " 'continent',\n",
    " ',',\n",
    " 'new',\n",
    " 'nation',\n",
    " ',',\n",
    " 'conceived',\n",
    " 'liberty',\n",
    " ',',\n",
    " 'dedicated',\n",
    " 'proposition',\n",
    " '``',\n",
    " 'men',\n",
    " 'created',\n",
    " 'equal',\n",
    " \"''\"]\n",
    "```\n",
    "\n",
    "To see the list of stop words in `nltk`, run the code below (there are 179). Then remove any word that is in sentences 2 & 3 AND in the stop word list (the list is alphabetic). \n",
    "\n",
    "The correct answer has 25 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_removed = [w for w in nltk.tokenize.word_tokenize(gettysburg_address)[:36] if not w in stop_words] \n",
    "stop_words_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "This task attempts to take the tokens of the text and categorize them into pre-defined groups such as names or people, organizations, locations, times, monetary values, and so on.\n",
    "\n",
    "Run the code below to see the NER for the speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tree = nltk.ne_chunk(nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))[:36])\n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tree = nltk.ne_chunk(nltk.pos_tag(nltk.tokenize.word_tokenize(gettysburg_address))[37:78])\n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "Another popular package, in addition to `nltk`, is `spacy`. The render display of named entity recognition is much better in my opinion but it only found two named entities using default options.\n",
    "\n",
    "Here is the screen shot of the analysis:\n",
    "\n",
    "![spacy](./spacy_gburg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python37064bitbaseconda74f6bf482b3748dfab216fe046a8ef20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
